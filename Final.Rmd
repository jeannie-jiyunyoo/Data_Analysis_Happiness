---
title: "Data Analysis on the Most Significant Factor of Happiness Around the Globe"
author: Jiyun Yoo
date: "4/20/2022"
output:
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(magrittr)
library(DataExplorer)
```

# Introduction

With the advent of Covid-19 over the past few years, the devastation of the pandemic such as millions of deaths, economic strife and unprecedented restrictions on social interaction has significantly affected on people's mental health. Researchers and scientists worldwide has been analyzing the impact of this despair and uncertainty and claim that "the deterioration in mental health could linger long after the pandemic has subsided" (Nature News). As the world is progressing day by day, our focus tend to shift towards endless developments rather than focusing on certain characteristics that actually matters daily, such as overlooking happiness. While the research claims that the current mental health status could linger long after pandemic, it is significant to understand the determining factors of happiness and how they affect the well-being of individuals. Thus, as a society, people can focus on certain factors together to form a happier environment. The "World's Happiness Report 2021" data from Kaggle was utilized for analysis. The data consists of total 149 inputs on different countries in the world. Through this research and data analysis, we will ascertain the answer to the question of "What are the most significant factors of happiness all over the globe?" to contribute positive impact of particular control measures to changes in people's well-being and to apprise the management of future pandemics.

### EDA

```{r}
data <- read.csv("~/Desktop/Final Project/archive/world-happiness-report-2021.csv")

# Replacing the column names with proper names
names(data) <- gsub("\\.", "_", names(data)) %>%
    gsub("___|__", "_", .)
data <- data %>%
    tibble::column_to_rownames("Country_name")
data <- data %>%
    dplyr::select(2, 1, 3:ncol(data))

# EDA
# Check null values
sapply(data, function(x) sum(is.na(x))) %>%
    as.data.frame() %>%
    set_colnames("NA_count") %>%
    knitr::kable()
plot_missing(data, ggtheme = theme_bw())

# Check categorical variable
plot_bar(data[,c("Regional_indicator")], ggtheme = theme_bw())
```

```{r fig.width=15, fig.height=7}
# Check density and distributions of continuous variables
par(mfrow = c(3, 6))
for (i in 3:ncol(data)) {
    hist(data[,i], breaks = "FD", main = names(data)[i])
}
```

+ There are 2 categorical and 18 continuous variables in the dataset
+ There are 149 countries in the dataset
+ No missing value exists in the dataset
+ 0 duplicate countries in the dataset


```{r}
hist(data$Ladder_score)
shapiro.test(data$Ladder_score) # As p-value > 0.05, the outcome is normally distributed
```




```{r results='hide'}
# Train / test set split 7:3
set.seed(101)
tr_idx <- sort(sample(1:nrow(data), round(nrow(data) * 0.7), replace = F))
train <- data[tr_idx,]
test <- data[-tr_idx,]

# Modeling
# (1) Full model
fit_lm_1 <- lm(Ladder_score ~ ., train)
summary(fit_lm_1)

# (2) Variable section
# - forward selection
start_mod <- lm(Ladder_score ~ 1, train)
empty_mod <- lm(Ladder_score ~ 1, train)
full_mod <- lm(Ladder_score ~ ., train)

library(stats)
forwardStepwise <- step(start_mod, 
                        scope = list(upper = full_mod,
                                     lower = empty_mod),
                        direction = "forward")
summary(forwardStepwise)
forward_selection_vars <- c("lowerwhisker", "upperwhisker", "Dystopia_residual")

# - backward selection
start_mod <- lm(Ladder_score ~ ., train)
empty_mod <- lm(Ladder_score ~ 1, train)
full_mod <- lm(Ladder_score ~ 1, train)

backwardStepwise <- step(start_mod, 
                        scope = list(upper = full_mod,
                                     lower = empty_mod),
                        direction = "backward")
summary(backwardStepwise)
backward_selection_vars <- c("upperwhisker", "lowerwhisker", "Logged_GDP_per_capita", "Healthy_life_expectancy", "Generosity", "Explained_by_Social_support", "Explained_by_Freedom_to_make_life_choices", "Explained_by_Perceptions_of_corruption", "Dystopia_residual")

# - stepwise selection
start_mod <- lm(Ladder_score ~ 1, train)
empty_mod <- lm(Ladder_score ~ 1, train)
full_mod <- lm(Ladder_score ~ ., train)

hybridStepwise = step(start_mod, 
                        scope = list(upper = full_mod,
                                     lower = empty_mod),
                        direction = "both")
summary(hybridStepwise)
# same with forward selection variables
```

### Forward selection model

```{r}
summary(forwardStepwise)
```

### Backward selection model

```{r}
summary(backwardStepwise)
```

### Stepwise selection model

```{r}
summary(hybridStepwise)
```


### Model Improvement and assumption check

```{r}
# As stepwise selection method is the same with forward selection, now we are only cmoparing two models: forward vs. backward

# Forward vs. backward
fit_forward <- lm(Ladder_score ~ ., train[,c("Ladder_score", forward_selection_vars)])
fit_backward <- lm(Ladder_score ~ ., train[,c("Ladder_score", backward_selection_vars)])

# Multicollinearity
library(car)
vif(fit_forward) # Either lowerwhisker or upperwhisker should be removed from the model and be refitted.
vif(fit_backward) # Too much multicollinearity exists in the model between predictors, this is not a good model.

# To resolve multicollinearity, 
# - upperwhisker was removed from the forward selection model
# - upperwhisker and lowerwhisker were removed from the backward selection model
# All the VIF of the predictors for each model < 10.
fit_forward2 <- lm(Ladder_score ~ lowerwhisker + Dystopia_residual, train)
summary(fit_forward2)
vif(fit_forward2)

fit_backward2 <- lm(Ladder_score ~ ., train[,c("Ladder_score", backward_selection_vars)] %>% dplyr::select(-lowerwhisker, -upperwhisker))
summary(fit_backward2)
vif(fit_backward2)

# Diagnostics: normality, constant variance (homoskedasticity), outliers
par(mfrow = c(2,2))
plot(fit_forward2)

par(mfrow = c(2,2))
plot(fit_backward2)

# For the forward selection model, although normality assumption was not satisfied, constant variance assumption was met. For the backward selection model, both normality assumption and constant variance assumption were both satisfied well. However, it would be better if we exclude the Rwanda observation from the analysis.

fit_backward3 <- lm(Ladder_score ~ ., train[-which(rownames(train) == "Rwanda"),c("Ladder_score", backward_selection_vars)] %>% dplyr::select(-lowerwhisker, -upperwhisker))
summary(fit_backward3)
plot(fit_backward3)

# # AIC
# AIC(fit_forward2)
# AIC(fit_backward3)
# 
# # adjusted R-squared
# summary(fit_forward2)$adj.r.squared
# summary(fit_backward3)$adj.r.squared

data.frame(VariableSelection = c("Forward2", "Backward3"),
           AIC = c(AIC(fit_forward2), AIC(fit_backward3)),
           adj_R_squared = c(summary(fit_forward2)$adj.r.squared, summary(fit_backward3)$adj.r.squared)) %>%
    knitr::kable()

# As Backward3's AIC is lower than that of the Forward2 and Backward's adjusted R-squared is greater than that of Forward2, our final model is "Ladder_score ~ Logged_GDP_per_capita + Healthy_life_expectancy + Generosity + Explained_by_Social_support + Explained_by_Freedom_to_make_life_choices + Explained_by_Perceptions_of_corruption + Dystopia_residual."
```


### Final Model

```{r}
fit_final <- lm(Ladder_score~Logged_GDP_per_capita + Healthy_life_expectancy + Generosity + Explained_by_Social_support + Explained_by_Freedom_to_make_life_choices + Explained_by_Perceptions_of_corruption + Dystopia_residual, train[-which(rownames(train) == "Rwanda"),])
summary(fit_final)

par(mfrow=c(2,2))
plot(fit_final)
```


### Research question

+ Research question: Which factor is the most contributing to increasing the happy score?
+ Answer: `Explained_by_Perceptions_of_corruption` is the most contributing factor to improve the score. (Inference)

+ (Prediction)
Can we predict other countries happy score using current existing dataset?
How accurate can our train model predict the ladder score of the countries in the test set?


### Model validation

Actual ladder score of the countries in the test set was compared to the predicted ladder score by trained final model. To evaluate the model performance, root mean squared error (RMSE) was used. As RMSE = 0.0008242059, which is very small, we can say that the final model is pretty robust.

```{r}
# Validation
pred <- predict(fit_final, test[,c("Logged_GDP_per_capita", "Healthy_life_expectancy", "Generosity", "Explained_by_Social_support", "Explained_by_Freedom_to_make_life_choices", "Explained_by_Perceptions_of_corruption", "Dystopia_residual")])

# library(Metrics)
# rmse(test$Ladder_score, pred)
(RMSE <- sqrt(mean((test$Ladder_score - pred)^2)))
```

```{r fig.width=15, fig.height=10}
library(effects)
plot(allEffects(fit_final))
```


